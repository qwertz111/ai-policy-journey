# EU AI Act ‚Äì Cheatsheet  

[üìù Glossar der wichtigsten Begriffe](glossary.md)

**12 Quick Facts**  
1. Definition ‚ÄûKI-System‚Äú  
2. Risikostufen  
3. Verbotene Praktiken  
4. High-Risk-Pflichten  
5. Konformit√§tsbewertung  
6. Markt√ºberwachung  
7. Transparenzauflagen  
8. Datengovernance  
9. Human Oversight  
10. Genauigkeit & Robustheit  
11. Aufzeichnungspflichten  
12. Durchsetzung & Bu√ügelder  

### Zweck des EU AI Acts (Art 1(1)
- Besseres Funktionieren des Binnenmarkts
- F√∂rderung einer **auf den Menschen ausgerichteten** KI
- Schutz der:
  - Gesundheit
  - Sicherheit
  - Grundrechte (Basis: Charta der Vereinten Nationen)
    - Demokratie
    - Rechtsstaatlichkeit
    - Umweltschutz
- Schutz vor sch√§dlichen Auswirkungen von KI-Systemen allgemein
- Unterst√ºtzung der Innovation

### KI-Campus ¬∑ AI Act Essentials ‚Äì Modul 1.1.1 ‚ÄûEine Technologie wie keine Andere‚Äú[^kc1]

| Merkmal | KI-System | Traditionelle Software |
|---------|-----------|------------------------|
| **Lernf√§higkeit** | Lernt aus Daten, passt Modelle an | Keine eigene Lernf√§higkeit ‚Äì folgt festem Code |
| **Verhalten √ºber Zeit** | Kann sich dynamisch √§ndern | Vorhersehbar; √§ndert sich nur bei Updates |
| **Weiterentwicklung** | Neues (Re-)Training m√∂glich, teils sogar im Feld | Erfordert manuellen Code-Eingriff & Release |
| **Unbekannte Szenarien** | Muss improvisieren ‚Äì Risiko falscher Reaktion bei Datenl√ºcken | Befolgt fixe Regeln, kann nicht improvisieren |

> **Kurz-Erl√§uterung:** KI-Systeme sind *datengetrieben* und entwickeln sich nach der Inbetriebnahme weiter. Das erkl√§rt die strengen Vorgaben zu **Risikomanagement (Art. 9)** und **Post-Market-Monitoring (Art. 61 ff.)** im EU AI Act.

[^kc1]: Quelle: KI-Campus Online-Kurs *AI Act Essentials*, Modul 1 ‚Äì 2025 ¬∑ Lizenz CC BY-SA 4.0

----------------------------------------------------------------------------
----------------------------------------------------------------------------

## Art. 5 ‚Äì Verbotene AI-Praktiken

### √úbersicht zu Artikel 5 ‚Äì Verbotene KI-Praktiken

| Buchstabe | Verboten, wenn das KI-System ‚Ä¶ | Alltagsbeispiel |
|-----------|--------------------------------|-----------------|
| **(a)** | unterbewusste Manipulation oder T√§uschung nutzt, sodass Menschen Entscheidungen treffen, die ihnen schaden k√∂nnen. | Shopping-App, die Kinder mit versteckten Reizen zu Impulsk√§ufen bewegt. |
| **(b)** | gezielt Schwachstellen einer bestimmten Gruppe (Kinder, Menschen mit Behinderung ‚Ä¶) ausnutzt und dadurch ihr Verhalten verzerrt. | Spiel, das Sehbehinderte mit speziellen Anreizen in eine Kaufsucht treibt. |
| **(c)** | ‚ÄûSocial Scoring‚Äú durchf√ºhrt und daraus Benachteiligungen entstehen. | Vermietungs-Scoring, das Party-Fotos aus Social Media gegen Bewerber*innen verwendet. |
| **(d)** | mittels Profiling vorhersagt, ob eine Person k√ºnftig eine Straftat begeht. | Software, die aus Chat-Verhalten eine Kriminalit√§tsprognose ableitet. |
| **(e)** | massenhaft Gesichtsbilder aus dem Netz sammelt, um Datenbanken aufzubauen. | Web-Crawler, der Profilfotos f√ºr eine globale Face-DB erntet. |
| **(f)** | Emotionserkennung in Arbeitsplatz, Schule oder √§hnlichen sensiblen Bereichen einsetzt. | Kamera im Klassenzimmer, die ‚ÄûAufmerksamkeit‚Äú der Sch√ºler meldet. |
| **(g)** | biometrische Kategorisierung nutzt, um sensible Merkmale (Religion, Sexualit√§t ‚Ä¶) abzuleiten. | Automat, der aus einem Selfie politische Gesinnung sch√§tzt. |
| **(h)** | biometrische Echtzeit-Fernidentifizierung in √∂ffentlichen R√§umen zu Polizeizwecken einsetzt (nur sehr enge Ausnahmen). | Gesichtserkennung auf dem Bahnhofsvorplatz ohne richterliche Genehmigung. |

**Strafen bei Verst√∂√üen:** bis zu 35 Mio. ‚Ç¨ oder 7 % des weltweiten Jahresumsatzes (Art. 99).  

### Vereinfachte Compliance-Checkliste Art. 5 EU AI Act
- [ ] **(1)(a)** Nutzt das System unterschwellige Techniken, um Individuen zu manipulieren?
- [ ] **(1)(b)** Nutzt das System Schwachstellen spezifischer Gruppen (z. B. Kinder, Menschen mit Behinderung) aus, die zu physischen oder psychischen Sch√§den f√ºhren k√∂nnen?
- [ ] **(1)(c)** Beinhaltet das System soziale Bewertungen (‚ÄûSocial Scoring‚Äú) durch √∂ffentliche Stellen, die zu Benachteiligungen f√ºhren k√∂nnen?
- [ ] **(1)(d)** Nutzt das System Risikobewertungen, um vorherzusagen, ob eine Person eine Straftat begehen wird, basierend auf Profiling oder Pers√∂nlichkeitsmerkmalen?
- [ ] **(1)(f)** Nutzt das System Emotionserkennung am Arbeitsplatz, in Bildungseinrichtungen oder anderen sensiblen Kontexten?
- [ ] **(1)(h)** Nutzt das System biometrische Echtzeit-Fernidentifizierung zu Strafverfolgungszwecken?

----------------------------------------------------------------------------

## Art. 9 ‚Äì Risikomanagement  
‚Ä¶  
### Typische Bias-Quellen in KI-Systemen

| # | Bias | Kurzbeschreibung | Typische Ursache | Beispiel |
|---|------|-----------------|------------------|----------|
| 1 | **Sampling Bias** (Stichprobenverzerrung) | Trainingsdaten repr√§sentieren die Zielpopulation nicht ausreichend. | Daten stammen v. a. aus leicht zug√§nglichen Quellen oder bestimmten Regionen. | Gesichtserkennung, die √ºberwiegend mit Bildern hellh√§utiger Personen trainiert wurde. |
| 2 | **Selection Bias** | Bestimmte Gruppen werden systematisch h√§ufiger in den Daten ausgew√§hlt. | Nutzer:innen w√§hlen sich selbst in die Datenerhebung ein (Self-selection). | Empfehlungs¬≠system, das Filmbewertungen nur von aktiven Fans ber√ºcksichtigt. |
| 3 | **Historical Bias** | Vergangene gesellschaftliche Ungleichheiten spiegeln sich 1:1 in den Daten wider. | ‚ÄûAs-is‚Äú-Daten ohne Korrektur genutzt. | Einstellungs-KI √ºbernimmt fr√ºhere Diskriminierung gegen Frauen. |
| 4 | **Representation Bias** | Minderheiten oder Randgruppen sind unter- oder falsch repr√§sentiert. | Datenerhebung fokussiert Mainstream-Nutzer. | Sprachmodell versteht Dialekte schlechter als Hochsprache. |
| 5 | **Measurement Bias** | Ungeeignete Proxy-Variablen oder ungenaue Messmethoden. | Indirekte Features statt Zielgr√∂√üe gemessen. | Kredit-Scoring nutzt Postleitzahl als Ersatz f√ºr Ausfallrisiko. |
| 6 | **Label Bias** | Labels enthalten subjektive oder inkonsistente Urteile. | Menschliche Annotator:innen interpretieren Kriterien unterschiedlich. | Hate-Speech-Dataset mit uneinheitlicher Definition von ‚Äûtoxisch‚Äú. |
| 7 | **Algorithmic Bias** | Lernverfahren verst√§rkt vorhandene Ungleichheiten oder verzerrt Feature-Gewichte. | Loss-Funktion ohne Fairness-Regularisierung. | Klassifikator priorisiert das Mehrheitslabel, weil es Genauigkeit maximiert. |
| 8 | **Confirmation Bias** | Entwickler suchen (unbewusst) nach Ergebnissen, die ihre Annahmen best√§tigen. | Hypothesen-getriebene Feature-Selektion. | Nur Features behalten, die erwartete Korrelation zeigen. |
| 9 | **Temporal Bias** | Datenbasis veraltet, spiegelt aktuelle Realit√§t nicht mehr wider. | Lange Iterationszyklen, seltenes Retraining. | Nachfrage-Forecast ignoriert ver√§ndertes Kaufverhalten nach einer Krise. |
| 10 | **Automation Bias** | Nutzer:innen vertrauen KI-Ausgabe √ºberm√§√üig und hinterfragen sie nicht. | ‚ÄûBlack-box‚Äú-Systeme ohne Erkl√§rbarkeit. | Radiologe √ºbersieht Fehlklassifikation, weil das System ‚Äû99 % sicher‚Äú meldet. |

> **Hinweis:** Mehrere Bias-Arten k√∂nnen gleichzeitig auftreten. Ein robustes **Risikomanagement (Art. 9 EU AI Act)** sollte deshalb Datenerhebung, Modellierung und Nutzung ganzheitlich pr√ºfen ‚Äì inklusive regelm√§√üiger **Fairness-Metriken** (z. B. Demographic Parity, Equalized Odds) und **Post-Market-Monitoring**.

