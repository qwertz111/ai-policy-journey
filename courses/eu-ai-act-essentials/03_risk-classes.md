
# Risikoklassen

## Inakzeptables Risiko (Art. 5)
Mit der Checkliste gemäß Art. 5 lässt sich schnell feststellen, ob ein KI-System untersagt ist. Trifft mindestens ein Punkt zu („Ja“), darf das System nicht entwickelt oder eingesetzt werden – außer unter den engen Ausnahmen des Gesetzes.

### Checkliste für inakzeptables Risiko
- [ ] **(1)(a)** Unterschwellige Techniken zur Manipulation von Individuen  
- [ ] **(1)(b)** Ausnutzung von Schwachstellen spezifischer Gruppen (z. B. Kinder, Menschen mit Behinderung) mit potenziell physischen oder psychischen Schäden  
- [ ] **(1)(c)** Soziales Scoring durch öffentliche Stellen mit Benachteiligungsgefahr  
- [ ] **(1)(d)** Risikobewertung zur Vorhersage kriminellen Verhaltens auf Basis von Profiling oder Persönlichkeitsmerkmalen  
- [ ] **(1)(f)** Einsatz von Emotionserkennung in sensiblen Kontexten (Arbeitsplatz, Bildung)  
- [ ] **(1)(h)** Biometrische Echtzeit-Fernidentifikation zu Strafverfolgungszwecken  

---

## Hohes Risiko (Art. 6)
Art. 6 definiert KI-Systeme mit hohem Risiko, konkretisiert in Art. 6 § 1 (a)/(b), § 2 und Anhang III.

### Checkliste: Kategorien (Anhang III)
> Fällt Ihr KI-System unter mindestens eine der folgenden Kategorien? Dann gilt es als hohes Risiko.

- [ ] **(1)** Biometrische Datenverarbeitung (sofern rechtlich zulässig)  
- [ ] **(2)** Sicherheitskomponenten in kritischen Infrastrukturen  
  - Straßenverkehr  
  - Wasser-, Gas-, Wärme- oder Stromversorgung  
- [ ] **(3)** Allgemeine und berufliche Bildung  
  - Zugang oder Zulassung zu Bildungseinrichtungen  
  - Bewertung von Lernergebnissen und Bildungsniveau  
  - Prüfungsüberwachung  
- [ ] **(4)** Beschäftigung, Arbeitnehmermanagement & Selbstständigkeit  
  - Einstellung und Auswahl natürlicher Personen  
  - Entscheidungen zu Arbeitsbedingungen  
- [ ] **(5)** Zugang zu wesentlichen privaten Dienstleistungen & öffentlichen Diensten  
- [ ] **(6)** Strafverfolgung (rechtlich zulässig)  
- [ ] **(7)** Migrations-, Asyl- & Grenzkontrollmanagement (rechtlich zulässig)  
- [ ] **(8)** Rechtspflege & demokratische Prozesse  

### Maßnahmen für Anbieter/Betreiber (Auswahl)
- Risikomanagementsystem (Art. 9)  
- Datenqualität und –governance (Art. 10)  
- Transparenzpflichten (Art. 13)  
- Menschliche Aufsicht (Art. 14)  

---

## Begrenztes Risiko (Art. 50)
KI-Systeme außerhalb der Hochrisiko-Kategorien ohne erhebliche Auswirkungen auf Grundrechte, Gesundheit oder Sicherheit, aber mit „Transparenz-Risiken“ (Täuschung, Manipulation oder undeklarierte KI-Generierung).

### Typische Anwendungsbeispiele
- Chat- & Voice-Bots  
- Generative KI (Text-, Bild-, Audio-, Video-Generatoren)  
- Deepfake-Generatoren  
- Emotionserkennung und biometrische Kategorisierung  

### Transparenzpflichten (Art. 50)
- Nutzer*innen müssen klar und rechtzeitig informiert werden, dass sie mit KI interagieren  
- Generative KI-Ausgaben müssen technisch erkennbar sein (Maschinenlesetag, ggf. Wasserzeichen)  
- Deepfakes und KI-Texte für öffentliche Meinungsbildung müssen deutlich gekennzeichnet sein  
- Betreiber müssen auf Emotionserkennung und biometrische Analysen hinweisen  

---

## Geringes Risiko
KI-Anwendungen mit geringem oder keinem Risiko für Gesundheit, Sicherheit oder Grundrechte.

### Beispiele
- Spam-Filter  
- Empfehlungsalgorithmen (E-Commerce, Streaming)  
- KI-Features in Videospielen  
- Automatische E-Mail-Sortierung  

### Pflichten & Empfehlungen
- Keine speziellen Pflichten im AI Act  
- Freiwillige Verhaltenskodizes (z. B. AI Pact, Code of Practice) zur Sicherstellung von Datensicherheit, Transparenz und Bias-Tests  
